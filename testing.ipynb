{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8ec03a-0304-4fd5-bec5-e0842067561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from architectures_28x28.CKAN_BN import CKAN_BN\n",
    "from architectures_28x28.SimpleModels import *\n",
    "from architectures_28x28.ConvNet import ConvNet\n",
    "from architectures_28x28.KANConvs_MLP import KANC_MLP\n",
    "from architectures_28x28.KKAN import KKAN_Convolutional_Network\n",
    "from architectures_28x28.conv_and_kan import NormalConvsKAN\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "device =\"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5785ac98-f496-45e8-a0cc-691b1e3861c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformaciones\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(cifar10_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(cifar10_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93460f73-e0d4-478a-9447-dc24b6ba2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credits to: https://github.com/detkov/Convolution-From-Scratch/\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "\n",
    "def calc_out_dims(matrix, kernel_side, stride, dilation, padding):\n",
    "    batch_size,n_channels,n, m = matrix.shape\n",
    "    h_out = np.floor((n + 2 * padding[0] - kernel_side - (kernel_side - 1) * (dilation[0] - 1)) / stride[0]).astype(int) + 1\n",
    "    w_out = np.floor((m + 2 * padding[1] - kernel_side - (kernel_side - 1) * (dilation[1] - 1)) / stride[1]).astype(int) + 1\n",
    "    b = [kernel_side // 2, kernel_side// 2]\n",
    "    return h_out,w_out,batch_size,n_channels\n",
    "\n",
    "def kan_conv2d(matrix: Union[List[List[float]], np.ndarray], #but as torch tensors. Kernel side asume q el kernel es cuadrado\n",
    "             kernel, \n",
    "             kernel_side,\n",
    "             stride= (1, 1), \n",
    "             dilation= (1, 1), \n",
    "             padding= (0, 0),\n",
    "             device= \"cuda\"\n",
    "             ) -> torch.Tensor:\n",
    "    \"\"\"Makes a 2D convolution with the kernel over matrix using defined stride, dilation and padding along axes.\n",
    "\n",
    "    Args:\n",
    "        matrix (batch_size, colors, n, m]): 2D matrix to be convolved.\n",
    "        kernel  (function]): 2D odd-shaped matrix (e.g. 3x3, 5x5, 13x9, etc.).\n",
    "        stride (Tuple[int, int], optional): Tuple of the stride along axes. With the `(r, c)` stride we move on `r` pixels along rows and on `c` pixels along columns on each iteration. Defaults to (1, 1).\n",
    "        dilation (Tuple[int, int], optional): Tuple of the dilation along axes. With the `(r, c)` dilation we distancing adjacent pixels in kernel by `r` along rows and `c` along columns. Defaults to (1, 1).\n",
    "        padding (Tuple[int, int], optional): Tuple with number of rows and columns to be padded. Defaults to (0, 0).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2D Feature map, i.e. matrix after convolution.\n",
    "    \"\"\"\n",
    "    h_out, w_out,batch_size,n_channels = calc_out_dims(matrix, kernel_side, stride, dilation, padding)\n",
    "    \n",
    "    matrix_out = torch.zeros((batch_size,n_channels,h_out,w_out)).to(device)#estamos asumiendo que no existe la dimension de rgb\n",
    "    unfold = torch.nn.Unfold((kernel_side,kernel_side), dilation=dilation, padding=padding, stride=stride)\n",
    "\n",
    "\n",
    "    for channel in range(n_channels):\n",
    "        #print(matrix[:,channel,:,:].unsqueeze(1).shape)\n",
    "        conv_groups = unfold(matrix[:,channel,:,:].unsqueeze(1)).transpose(1, 2)\n",
    "        #print(\"conv\",conv_groups.shape)\n",
    "        for k in range(batch_size):\n",
    "            matrix_out[k,channel,:,:] = kernel.forward(conv_groups[k,:,:]).reshape((h_out,w_out))\n",
    "    return matrix_out\n",
    "\n",
    "def multiple_convs_kan_conv2d(matrix, #but as torch tensors. Kernel side asume q el kernel es cuadrado\n",
    "             kernels, \n",
    "             kernel_side,\n",
    "             stride= (1, 1), \n",
    "             dilation= (1, 1), \n",
    "             padding= (0, 0),\n",
    "             device= \"cuda\"\n",
    "             ) -> torch.Tensor:\n",
    "    \"\"\"Makes a 2D convolution with the kernel over matrix using defined stride, dilation and padding along axes.\n",
    "\n",
    "    Args:\n",
    "        matrix (batch_size, colors, n, m]): 2D matrix to be convolved.\n",
    "        kernel  (function]): 2D odd-shaped matrix (e.g. 3x3, 5x5, 13x9, etc.).\n",
    "        stride (Tuple[int, int], optional): Tuple of the stride along axes. With the `(r, c)` stride we move on `r` pixels along rows and on `c` pixels along columns on each iteration. Defaults to (1, 1).\n",
    "        dilation (Tuple[int, int], optional): Tuple of the dilation along axes. With the `(r, c)` dilation we distancing adjacent pixels in kernel by `r` along rows and `c` along columns. Defaults to (1, 1).\n",
    "        padding (Tuple[int, int], optional): Tuple with number of rows and columns to be padded. Defaults to (0, 0).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2D Feature map, i.e. matrix after convolution.\n",
    "    \"\"\"\n",
    "    h_out, w_out,batch_size,n_channels = calc_out_dims(matrix, kernel_side, stride, dilation, padding)\n",
    "    n_convs = len(kernels)\n",
    "    matrix_out = torch.zeros((batch_size,n_channels*n_convs,h_out,w_out)).to(device)#estamos asumiendo que no existe la dimension de rgb\n",
    "    unfold = torch.nn.Unfold((kernel_side,kernel_side), dilation=dilation, padding=padding, stride=stride)\n",
    "    conv_groups = unfold(matrix[:,:,:,:]).view(batch_size, n_channels,  kernel_side*kernel_side, h_out*w_out).transpose(2, 3)#reshape((batch_size,n_channels,h_out,w_out))\n",
    "    for channel in range(n_channels):\n",
    "        for kern in range(n_convs):\n",
    "            matrix_out[:,kern  + channel*n_convs,:,:] = kernels[kern].conv.forward(conv_groups[:,channel,:,:].flatten(0,1)).reshape((batch_size,h_out,w_out))\n",
    "    return matrix_out\n",
    "\n",
    "\n",
    "def add_padding(matrix: np.ndarray, \n",
    "                padding: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"Adds padding to the matrix. \n",
    "\n",
    "    Args:\n",
    "        matrix (np.ndarray): Matrix that needs to be padded. Type is List[List[float]] casted to np.ndarray.\n",
    "        padding (Tuple[int, int]): Tuple with number of rows and columns to be padded. With the `(r, c)` padding we addding `r` rows to the top and bottom and `c` columns to the left and to the right of the matrix\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Padded matrix with shape `n + 2 * r, m + 2 * c`.\n",
    "    \"\"\"\n",
    "    n, m = matrix.shape\n",
    "    r, c = padding\n",
    "    \n",
    "    padded_matrix = np.zeros((n + r * 2, m + c * 2))\n",
    "    padded_matrix[r : n + r, c : m + c] = matrix\n",
    "    \n",
    "    return padded_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa7eb4e-8f97-4239-985d-909bddb26967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from KANLinear import KANLinear\n",
    "import convolution\n",
    "\n",
    "\n",
    "#Script que contiene la implementación del kernel con funciones de activación.\n",
    "class KAN_Convolutional_Layer(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_convs: int = 1,\n",
    "            kernel_size: tuple = (2,2),\n",
    "            stride: tuple = (1,1),\n",
    "            padding: tuple = (0,0),\n",
    "            dilation: tuple = (1,1),\n",
    "            grid_size: int = 5,\n",
    "            spline_order:int = 3,\n",
    "            scale_noise:float = 0.1,\n",
    "            scale_base: float = 1.0,\n",
    "            scale_spline: float = 1.0,\n",
    "            base_activation=torch.nn.SiLU,\n",
    "            grid_eps: float = 0.02,\n",
    "            grid_range: tuple = [-1, 1],\n",
    "            device: str = \"cpu\"\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Kan Convolutional Layer with multiple convolutions\n",
    "        \n",
    "        Args:\n",
    "            n_convs (int): Number of convolutions to apply\n",
    "            kernel_size (tuple): Size of the kernel\n",
    "            stride (tuple): Stride of the convolution\n",
    "            padding (tuple): Padding of the convolution\n",
    "            dilation (tuple): Dilation of the convolution\n",
    "            grid_size (int): Size of the grid\n",
    "            spline_order (int): Order of the spline\n",
    "            scale_noise (float): Scale of the noise\n",
    "            scale_base (float): Scale of the base\n",
    "            scale_spline (float): Scale of the spline\n",
    "            base_activation (torch.nn.Module): Activation function\n",
    "            grid_eps (float): Epsilon of the grid\n",
    "            grid_range (tuple): Range of the grid\n",
    "            device (str): Device to use\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        super(KAN_Convolutional_Layer, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "        self.kernel_size = kernel_size\n",
    "        # self.device = device\n",
    "        self.dilation = dilation\n",
    "        self.padding = padding\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.n_convs = n_convs\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        # Create n_convs KAN_Convolution objects\n",
    "        for _ in range(n_convs):\n",
    "            self.convs.append(\n",
    "                KAN_Convolution(\n",
    "                    kernel_size= kernel_size,\n",
    "                    stride = stride,\n",
    "                    padding=padding,\n",
    "                    dilation = dilation,\n",
    "                    grid_size=grid_size,\n",
    "                    spline_order=spline_order,\n",
    "                    scale_noise=scale_noise,\n",
    "                    scale_base=scale_base,\n",
    "                    scale_spline=scale_spline,\n",
    "                    base_activation=base_activation,\n",
    "                    grid_eps=grid_eps,\n",
    "                    grid_range=grid_range,\n",
    "                    # device = device ## changed device to be allocated as per the input device for pytorch DDP\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "        # If there are multiple convolutions, apply them all\n",
    "        self.device = x.device\n",
    "        if self.n_convs>1:\n",
    "            return convolution.multiple_convs_kan_conv2d(x, self.convs,self.kernel_size[0],self.stride,self.dilation,self.padding,self.device)\n",
    "        \n",
    "        # If there is only one convolution, apply it\n",
    "        return self.convs[0].forward(x)\n",
    "        \n",
    "\n",
    "class KAN_Convolution(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            kernel_size: tuple = (2,2),\n",
    "            stride: tuple = (1,1),\n",
    "            padding: tuple = (0,0),\n",
    "            dilation: tuple = (1,1),\n",
    "            grid_size: int = 5,\n",
    "            spline_order: int = 3,\n",
    "            scale_noise: float = 0.1,\n",
    "            scale_base: float = 1.0,\n",
    "            scale_spline: float = 1.0,\n",
    "            base_activation=torch.nn.SiLU,\n",
    "            grid_eps: float = 0.02,\n",
    "            grid_range: tuple = [-1, 1],\n",
    "            device = \"cpu\"\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        \"\"\"\n",
    "        super(KAN_Convolution, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        # self.device = device\n",
    "        self.conv = KANLinear(\n",
    "            in_features = math.prod(kernel_size),\n",
    "            out_features = 1,\n",
    "            grid_size=grid_size,\n",
    "            spline_order=spline_order,\n",
    "            scale_noise=scale_noise,\n",
    "            scale_base=scale_base,\n",
    "            scale_spline=scale_spline,\n",
    "            base_activation=base_activation,\n",
    "            grid_eps=grid_eps,\n",
    "            grid_range=grid_range\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "        self.device = x.device\n",
    "        return convolution.kan_conv2d(x, self.conv,self.kernel_size[0],self.stride,self.dilation,self.padding,self.device)\n",
    "    \n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        return sum( layer.regularization_loss(regularize_activation, regularize_entropy) for layer in self.layers)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c293b4f-fdd7-40a3-9eee-631df97a9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        return base_output + spline_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)\n",
    "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as stated in the\n",
    "        paper, since the original one requires computing absolutes and entropy from the\n",
    "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
    "        behind the F.linear function if we want an memory efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the spline\n",
    "        weights. The authors implementation also includes this term in addition to the\n",
    "        sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n",
    "\n",
    "\n",
    "class KAN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_hidden,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KAN, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
    "            self.layers.append(\n",
    "                KANLinear(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    grid_size=grid_size,\n",
    "                    spline_order=spline_order,\n",
    "                    scale_noise=scale_noise,\n",
    "                    scale_base=scale_base,\n",
    "                    scale_spline=scale_spline,\n",
    "                    base_activation=base_activation,\n",
    "                    grid_eps=grid_eps,\n",
    "                    grid_range=grid_range,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "        for layer in self.layers:\n",
    "            if update_grid:\n",
    "                layer.update_grid(x)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        return sum(\n",
    "            layer.regularization_loss(regularize_activation, regularize_entropy)\n",
    "            for layer in self.layers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dcda65a-0915-4435-a9c9-5f27737e7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10KANModel(nn.Module):\n",
    "    def __init__(self,device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.conv1 = KAN_Convolutional_Layer(\n",
    "            n_convs = 5,\n",
    "            kernel_size= (3,3),\n",
    "            device = device\n",
    "        )\n",
    "\n",
    "        self.conv2 = KAN_Convolutional_Layer(\n",
    "            n_convs = 5,\n",
    "            kernel_size = (3,3),\n",
    "            device = device\n",
    "        )\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(\n",
    "            kernel_size=(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.flat = nn.Flatten() \n",
    "        \n",
    "        self.linear1 = nn.Linear(2700, 256)\n",
    "        self.linear2 = nn.Linear(256, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69cafa46-9815-4211-9a11-06d35f281057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, criterion):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "\n",
    "    Args:\n",
    "        model: the neural network model\n",
    "        device: cuda or cpu\n",
    "        train_loader: DataLoader for training data\n",
    "        optimizer: the optimizer to use (e.g. SGD)\n",
    "        epoch: the current epoch\n",
    "        criterion: the loss function (e.g. CrossEntropy)\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: the average loss over the training set\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    # print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Test the model\n",
    "\n",
    "    Args:\n",
    "        model: the neural network model\n",
    "        device: cuda or cpu\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: the loss function (e.g. CrossEntropy)\n",
    "\n",
    "    Returns:\n",
    "        test_loss: the average loss over the test set\n",
    "        accuracy: the accuracy of the model on the test set\n",
    "        precision: the precision of the model on the test set\n",
    "        recall: the recall of the model on the test set\n",
    "        f1: the f1 score of the model on the test set\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (target == predicted).sum().item()\n",
    "\n",
    "            # Collect all targets and predictions for metric calculations\n",
    "            all_targets.extend(target.view_as(predicted).cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    precision = precision_score(all_targets, all_predictions, average='macro')\n",
    "    recall = recall_score(all_targets, all_predictions, average='macro')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "    # Normalize test loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "    # print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}\\n'.format(\n",
    "    #     test_loss, correct, len(test_loader.dataset), accuracy, precision, recall, f1))\n",
    "\n",
    "    return test_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ad5dd0c-6a88-4d50-b8ef-8904b004a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_models(model, device, train_loader, test_loader, optimizer, criterion, epochs, scheduler):\n",
    "    \"\"\"\n",
    "    Train and test the model\n",
    "\n",
    "    Args:\n",
    "        model: the neural network model\n",
    "        device: cuda or cpu\n",
    "        train_loader: DataLoader for training data\n",
    "        test_loader: DataLoader for test data\n",
    "        optimizer: the optimizer to use (e.g. SGD)\n",
    "        criterion: the loss function (e.g. CrossEntropy)\n",
    "        epochs: the number of epochs to train\n",
    "        scheduler: the learning rate scheduler\n",
    "\n",
    "    Returns:\n",
    "        all_train_loss: a list of the average training loss for each epoch\n",
    "        all_test_loss: a list of the average test loss for each epoch\n",
    "        all_test_accuracy: a list of the accuracy for each epoch\n",
    "        all_test_precision: a list of the precision for each epoch\n",
    "        all_test_recall: a list of the recall for each epoch\n",
    "        all_test_f1: a list of the f1 score for each epoch\n",
    "    \"\"\"\n",
    "    # Track metrics\n",
    "    all_train_loss = []\n",
    "    all_test_loss = []\n",
    "    all_test_accuracy = []\n",
    "    all_test_precision = []\n",
    "    all_test_recall = []\n",
    "    all_test_f1 = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train the model\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch, criterion)\n",
    "        all_train_loss.append(train_loss)\n",
    "        \n",
    "        # Test the model\n",
    "        test_loss, test_accuracy, test_precision, test_recall, test_f1 = test(model, device, test_loader, criterion)\n",
    "        all_test_loss.append(test_loss)\n",
    "        all_test_accuracy.append(test_accuracy)\n",
    "        all_test_precision.append(test_precision)\n",
    "        all_test_recall.append(test_recall)\n",
    "        all_test_f1.append(test_f1)\n",
    "\n",
    "        print(f'End of Epoch {epoch}: Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2%}')\n",
    "        scheduler.step()\n",
    "    model.all_test_accuracy = all_test_accuracy\n",
    "    model.all_test_precision = all_test_precision\n",
    "    model.all_test_f1 = all_test_f1\n",
    "    model.all_test_recall = all_test_recall\n",
    "\n",
    "    return all_train_loss, all_test_loss, all_test_accuracy, all_test_precision, all_test_recall, all_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0642cb-9049-4601-8168-dfe23ae2e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ffbbc-1fce-4c01-89b1-076ba3629fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 782/782 [3:10:02<00:00, 14.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 1: Train Loss: 1.532792, Test Loss: 0.0207, Accuracy: 54.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▏                                                                       | 71/782 [19:00<2:51:18, 14.46s/it]"
     ]
    }
   ],
   "source": [
    "model = CIFAR10KANModel().to(device)\n",
    "optimizer_Convs_and_KAN = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler_Convs_and_KAN = optim.lr_scheduler.ExponentialLR(optimizer_Convs_and_KAN, gamma=0.8)\n",
    "criterion_Convs_and_KAN = nn.CrossEntropyLoss()\n",
    "\n",
    "all_train_loss_Convs_and_KAN, all_test_loss_Convs_and_KAN, all_test_accuracy_Convs_and_KAN, all_test_precision_Convs_and_KAN, all_test_recall_Convs_and_KAN, all_test_f1_Convs_and_KAN = train_and_test_models(model, device, train_loader, test_loader, optimizer_Convs_and_KAN, criterion_Convs_and_KAN, epochs=10, scheduler=scheduler_Convs_and_KAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9cf60-f1bd-45f2-a4c3-76525cb9e118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
